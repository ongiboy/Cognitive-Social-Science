{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Link to git repository: https://github.com/ongiboy/Cognitive-Social-Science\n",
    "\n",
    "Group member's contribution:\n",
    "* Every task was made in collaboration by all members."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 1: Web-scraping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "# Setup the URL\n",
    "LINK = \"https://ic2s2-2023.org/program\"\n",
    "r = requests.get(LINK)\n",
    "soup = BeautifulSoup(r.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extracting keynote speakers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "table = soup.find(\"table\",{\"class\":\"tutorials\"})\n",
    "table_rows = table.find_all(\"tr\")\n",
    "\n",
    "# For each row find the elements that include \"Keynote\" and save to list\n",
    "keynotes = []\n",
    "for tr in table_rows:\n",
    "    td = tr.find_all(\"td\")\n",
    "    row = [i.text for i in td]\n",
    "    if row:\n",
    "        if \"Keynote\" in row[1]:\n",
    "            keynotes.append(row[1])\n",
    "\n",
    "# Remove \"Keynote - \" from each element in list\n",
    "keynotes = [re.sub(\"Keynote - \", \"\", i) for i in keynotes]\n",
    "\n",
    "print(keynotes)\n",
    "print(\"Number of keynotes: \", len(list(set(keynotes))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extracting chairs people"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chairs = []\n",
    "# Find the sections with names\n",
    "sections = soup.find_all(\"h2\")\n",
    "for section in sections:\n",
    "\n",
    "    # Find names within section\n",
    "    bullets = section.find_all(\"i\")\n",
    "    for nameline in bullets:\n",
    "\n",
    "        # Add only names after \"Chairs\" to list (Chair: Taha Yasseri)\n",
    "        if \"Chair\" in nameline.text:\n",
    "            chairs.append(nameline.text[7:])\n",
    "        \n",
    "# unique elements in names\n",
    "print(chairs)\n",
    "print(\"Number of chairs: \", len(list(set(chairs))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extracting speakers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "names = []\n",
    "# Find the sections with names\n",
    "sections = soup.find_all(\"ul\",{\"class\":\"nav_list\"})\n",
    "for section in sections:\n",
    "\n",
    "    # Find names within section\n",
    "    bullets = section.find_all(\"i\")\n",
    "    for nameline in bullets:\n",
    "\n",
    "        # Split into each name and add to list\n",
    "        names.extend(nameline.text.split(\", \"))\n",
    "        \n",
    "print(names)\n",
    "print(\"Number of names: \", len(list(set(names))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Combined list of all researchers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine all lists into one\n",
    "all_names = keynotes + chairs + names\n",
    "\n",
    "# Remove duplicates\n",
    "all_names = list(set(all_names))\n",
    "\n",
    "print(\"Number of names: \", len(all_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save names in csv\n",
    "df = pd.DataFrame(all_names, columns=[\"name\"])\n",
    "df.to_csv(\"names.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How many unique researchers do you get?\n",
    "* 1491\n",
    "\n",
    "Explain the process you followed to web-scrape the page. \n",
    "Which choices did you make to accurately retreive as many names as possible? Which strategies did you use to assess the quality of your final list?\n",
    "* First, a detailed inspection of the page was made. The sections containing names were found:\n",
    "    * An overview table containing keynote speakers (< td >)\n",
    "    * Plenary talks containing chairs (< h2 >) and speakers (< i >)\n",
    "    * Parallel talks containing chairs (< h2 >) and speakers (< i >)\n",
    "    * Posters (< i >)\n",
    "* The sections were scraped for names. In the overview table, each row was searched to find the cells including \"Keynote\". Next in each section, the search was made in the subtitles < h2 >, where the italic text including the word \"Chairs\" was saved to a list. In the sections of plenary, parallel and poster talks the lines of the schedule were searched, in which the italic text was found and added to a list, where the names were split by \",\". \n",
    "* Finally the lists combined. \n",
    "* To ensure a better quality of the name list, potential duplicates were removed by turning the list into a set and then back to a list.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2: Ready Made vs Custom Made Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What are pros and cons of the custom-made data used in Centola's experiment (the first study presented in the lecture) and the ready-made data used in Nicolaides's study (the second study presented in the lecture)?\n",
    "* Centola's data samples and experimental setup were highly controlled, so the causal effect of clustered vs random networks on spread behavior could be investigated while avoiding confounding effects effectively. However, ethical aspects of transparency of the experimental data could be discussed, as the people registering to the health website were not informed about their participation of the experiment. The controlled set-up might also create an artificial environment not accurately reflecting clustered networks of the real world.  \n",
    "\n",
    "* The observational data from the fitness tracking app combined with the social network app used in Nicolaide's study, provided large samples under real world conditions, reflecting the users actual behaviors in their natural environments. However as control of data collecting were limited, potential biases in the app's user base can affect the generalizability of the findings. Other external variables may also affect the found correlation, such as the motivation or other explanations as homophily.  \n",
    "  \n",
    "\n",
    "How do you think these differences can influence the interpretation of the results in each study?\n",
    "* The findings of Centola's experiment are likely more internally valid because of the controlled setup which helps isolating the variables of interest and reduce external factors. This difference makes for a more clear understanding of the causal relationship between network structures and spread behavior. However, because of the artificial environment and lack of transparency, the results of Centolaâ€™s study may have limited external validity.\n",
    "* On the other side, the results of Nicolaide's study are more applicable to real-world scenarios, dealing with the complexity of actual social networks. However, because of the lack of control over the data collection there might be potential biases and confounding variables, making it necessary to interpret the findings with caution.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 3: Gathering Research Articles using the OpenAlex API"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define functions and import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "import tqdm\n",
    "import unicodedata\n",
    "\n",
    "def normalize_name(name):\n",
    "    \"\"\"\n",
    "    returns more standard version of name\n",
    "        (removes accents, special characters, and makes lowercase)\n",
    "    \"\"\"\n",
    "\n",
    "    normalized_name = ''.join(c for c in unicodedata.normalize('NFD', name) if unicodedata.category(c) != 'Mn' and (c.isalnum() or c.isspace()))\n",
    "    return normalized_name.lower()\n",
    "\n",
    "\n",
    "def get_info_from_names(df_names):\n",
    "    \"\"\"\n",
    "    Retrieves author information from names (through a search)\n",
    "    \"\"\"\n",
    "\n",
    "    BASE_URL = 'https://api.openalex.org'\n",
    "    RESOURCE = '/authors'\n",
    "\n",
    "    data = []\n",
    "    for i in tqdm.tqdm(range(len(df_names))):\n",
    "        try:\n",
    "            response = requests.get(BASE_URL + RESOURCE, params={\"search\": df_names[\"name\"][i]})\n",
    "            all_results = response.json()[\"results\"]#[0]\n",
    "\n",
    "            names = []\n",
    "            normalized_df_name = normalize_name(df_names[\"name\"][i])\n",
    "\n",
    "            for r in range(len(all_results)):\n",
    "                name = all_results[r][\"display_name\"]\n",
    "                names.append(normalize_name(name))\n",
    "\n",
    "                if normalized_df_name in names:\n",
    "                    results = response.json()[\"results\"][r]\n",
    "                    break\n",
    "\n",
    "            if normalized_df_name not in names:\n",
    "                continue\n",
    "\n",
    "            data += [[results[\"id\"], results[\"display_name\"], results[\"works_api_url\"], results[\"summary_stats\"][\"h_index\"], results[\"works_count\"], results[\"last_known_institution\"][\"country_code\"]]]\n",
    "\n",
    "        except (IndexError, KeyError, ValueError, TypeError) as e:\n",
    "            print(f\"Skipping data point at iteration {i} due to error: {e}\")\n",
    "            continue  # Skip the rest of the loop and proceed to the next iteration\n",
    "        except requests.exceptions.RequestException as e:\n",
    "            print(f\"Request error at iteration {i}: {e}\")\n",
    "            continue\n",
    "\n",
    "    new_df = pd.DataFrame(data, columns=[\"id\",\"display_name\",\"works_api_url\",\"h_index\",\"works_count\", \"country_code\"])\n",
    "    return new_df\n",
    "\n",
    "\n",
    "def get_concept_ids(concept_requirements):\n",
    "    \"\"\"\n",
    "    Retrieves the OpenAlex concept_ids for a list of requirements.\n",
    "    \"\"\"\n",
    "\n",
    "    BASE_URL = 'https://api.openalex.org'\n",
    "    RESOURCE = '/concepts'\n",
    "\n",
    "    concept_ids = []\n",
    "    for concept in concept_requirements:\n",
    "        result = requests.get(BASE_URL + RESOURCE, params={'search': concept, 'filter': 'level:0'}).json()\n",
    "        concept_ids.append(result['results'][0]['id'])\n",
    "\n",
    "    concept_ids = [id.split(\"/\")[-1] for id in concept_ids]\n",
    "    return concept_ids\n",
    "\n",
    "\n",
    "def get_articles_from_authors(names, concept_ids_requirements_1, concept_ids_requirements_2, subset=False):\n",
    "    \"\"\"\n",
    "    Extracts articles from authors in the names table.\n",
    "        The articles are filtered by the criteria from the assignment description.\n",
    "    \"\"\"\n",
    "    \n",
    "    BASE_URL = 'https://api.openalex.org'\n",
    "    RESOURCE = '/works'\n",
    "\n",
    "    # Filter out authors not having 5-5000 works\n",
    "    names = names[(names['works_count']>=5) & (names['works_count']<=5000)]\n",
    "\n",
    "    table1 = []\n",
    "    table2 = []\n",
    "\n",
    "    # Search for articles in batches of 25 authors\n",
    "    name_batches = [list(names['id'][i:i+25]) for i in range(0, len(names), 25)]\n",
    "\n",
    "    for num_name_batch, name_batch in tqdm.tqdm(enumerate(name_batches)):\n",
    "\n",
    "        # short version for testing\n",
    "        if subset and num_name_batch>0:\n",
    "            break\n",
    "\n",
    "        # Scroll through the results\n",
    "        cursor = '*'\n",
    "        while True:\n",
    "            filters = ['cited_by_count:>10', \n",
    "                        'authors_count:<10',\n",
    "                        'authorships.author.id:'+'|'.join(name_batch),\n",
    "                        'concepts.id:'+'|'.join(concept_ids_requirements_1),\n",
    "                        'concepts.id:'+'|'.join(concept_ids_requirements_2)\n",
    "                        ]\n",
    "            parameters = {'per-page': 200,\n",
    "                            'filter': ','.join(filters),\n",
    "                            'cursor': cursor\n",
    "                            }\n",
    "            result = requests.get(BASE_URL + RESOURCE, params=parameters).json()\n",
    "\n",
    "            # If last page is reached (which is empty), break \n",
    "            cursor = result['meta']['next_cursor'] # next page for next search\n",
    "            if len(result['results'])==0 or cursor is None:\n",
    "                break\n",
    "\n",
    "            # Go through all articles and extract information\n",
    "            for n_article,article in enumerate(result['results']):\n",
    "                try:\n",
    "                    tab1 = [article['id'], article['publication_year'], article['cited_by_count'], [author['author']['id'] for author in article['authorships']]]\n",
    "                    tab2 = [article['id'], article['title'], article['abstract_inverted_index']]\n",
    "                    table1.append(tab1)\n",
    "                    table2.append(tab2)\n",
    "\n",
    "                except:\n",
    "                    print(\"skipped name batch:\", num_name_batch, \"article:\", n_article)\n",
    "                    continue\n",
    "    \n",
    "    table1 = pd.DataFrame(table1, columns=['id', 'publication_year', 'cited_by_count', 'authors'])\n",
    "    table2 = pd.DataFrame(table2, columns=['id', 'title', 'abstract_inverted_index'])\n",
    "\n",
    "    return table1, table2\n",
    "\n",
    "\n",
    "def get_info_from_author_ids(author_ids):\n",
    "    \"\"\"\n",
    "    Gets info table from author ids.\n",
    "    \"\"\"\n",
    "\n",
    "    URL = 'https://api.openalex.org/authors'\n",
    "    author_ids = [id.split('/')[-1] for id in author_ids]\n",
    "\n",
    "    co_author_info = []\n",
    "    id_batches = [author_ids[i:i+25] for i in range(0, len(author_ids), 25)]\n",
    "\n",
    "    for i,ids_batch in tqdm.tqdm(enumerate(id_batches)):\n",
    "            \n",
    "        # search for result\n",
    "        params = {'filter': 'ids.openalex:'+'|'.join(ids_batch)}\n",
    "        result_batch = requests.get(URL, params=params).json()\n",
    "\n",
    "        # No results\n",
    "        if len(result_batch) == 0:\n",
    "            continue\n",
    "        \n",
    "        # Go through results (authors)\n",
    "        for result in result_batch['results']:\n",
    "\n",
    "            # if person doesnt have all info, skip person\n",
    "            try:\n",
    "                # extract desired information\n",
    "                person_info = [result['id'], result['display_name'], result['works_api_url'], result['summary_stats']['h_index'], result['works_count'], result['last_known_institution']['country_code']]\n",
    "            except:\n",
    "                continue\n",
    "\n",
    "            co_author_info.append(person_info)\n",
    "    \n",
    "    co_author_info_df = pd.DataFrame(co_author_info, columns=['id', 'display_name', 'works_api_url', 'h_index', 'works_count', 'country_code'])\n",
    "\n",
    "    return co_author_info_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, turn names-list from Part 1 into a pandas dataframe\n",
    "names = pd.read_csv('data/names.csv')\n",
    "\n",
    "# Get info from names (table of authors + info)\n",
    "authors = get_info_from_names(names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop duplicates\n",
    "authors = authors.drop_duplicates(subset='id')\n",
    "\n",
    "# Save authors to csv\n",
    "authors.to_csv(\"data/authors.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Authors from week 2 is downloaded\n",
    "\n",
    "(workflow in different file: \"data_preperation.ipynb\" since its is not part of the task)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "authors = pd.read_csv('data/authors_final.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, the concept_ids from the desired concepts are retrieved (for use in the search)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get concept ids\n",
    "concepts_requirements_1 = ['Sociology', 'Psychology', 'Economics', 'Political Science']\n",
    "concepts_requirements_2 = ['Mathematics', 'Physics', 'Computer Science']\n",
    "concept_ids_1 = get_concept_ids(concepts_requirements_1)\n",
    "concept_ids_2 = get_concept_ids(concepts_requirements_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, the articles by the author list are retrieved, using the specified filters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get articles made by names\n",
    "authors = pd.read_csv('data/authors_final.csv')\n",
    "\n",
    "articles_from_authors, abstracts_from_authors = get_articles_from_authors(authors, concept_ids_1, concept_ids_2, subset=False)\n",
    "print(\"Number of articles:\", len(articles_from_authors))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, duplicated are dropped and files are saved."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop duplicates from articles and abstracts\n",
    "papers = articles_from_authors.drop_duplicates(subset=['id'])\n",
    "abstracts = abstracts_from_authors.drop_duplicates(subset=['id'])\n",
    "\n",
    "# Save articles to csv\n",
    "papers.to_csv(\"data/papers.csv\", index=False)\n",
    "abstracts.to_csv(\"data/abstracts.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (OBS: Following not asked in assignment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load papers and authors\n",
    "import ast\n",
    "authors = pd.read_csv('data/authors_final.csv')\n",
    "papers = pd.read_csv('data/papers.csv')\n",
    "\n",
    "author_ids = authors[\"id\"]\n",
    "co_authors_ids = [x for x in papers[\"authors\"].copy().apply(ast.literal_eval).explode().dropna().unique() if x not in author_ids]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get info from author ids\n",
    "co_author_info = get_info_from_author_ids(co_authors_ids)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop rows in co_author_info if \"nan\" in country_code\n",
    "co_author_info = co_author_info.dropna(subset=['country_code'])\n",
    "\n",
    "# Concatenate the authors and co_author_info tables\n",
    "co_authors_df = pd.concat([authors, co_author_info]).drop_duplicates(subset=['id'])\n",
    "\n",
    "# Drop duplicates\n",
    "co_author_info = co_author_info.drop_duplicates(subset='id')\n",
    "\n",
    "# Save authors to csv\n",
    "co_author_info.to_csv(\"data/co_authors_info.csv\", index=False)\n",
    "co_authors_df.to_csv(\"data/THE_AUTHOR_DATASET.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get papers and abstracts from co-authors (We use the same concept_ids as earlier)\n",
    "papers_from_co_authors, abstracts_from_co_authors = get_articles_from_authors(co_author_info, concept_ids_1, concept_ids_2, subset=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concatenate the papers and abstracts tables\n",
    "papers = pd.read_csv('data/papers.csv')\n",
    "abstracts = pd.read_csv('data/abstracts.csv')\n",
    "\n",
    "papers_final = pd.concat([papers, papers_from_co_authors]).drop_duplicates(subset=['id'])\n",
    "abstracts_final = pd.concat([abstracts, abstracts_from_co_authors]).drop_duplicates(subset=['id'])\n",
    "\n",
    "# Save papers and abstracts to csv\n",
    "papers_final.to_csv(\"data/THE_PAPER_DATASET.csv\", index=False)\n",
    "abstracts_final.to_csv(\"data/THE_ABSTRACT_DATASET.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "128471it [02:00, 1064.44it/s]\n"
     ]
    }
   ],
   "source": [
    "import ast\n",
    "all_papers = pd.read_csv(\"data/THE_PAPER_DATASET.csv\")\n",
    "co_authors_df = pd.read_csv(\"data/THE_AUTHOR_DATASET.csv\")\n",
    "\n",
    "# Drop duplicates from papers and abstracts\n",
    "all_papers = all_papers.drop_duplicates(subset=['id'])\n",
    "\n",
    "all_papers_copy = all_papers.copy()\n",
    "\n",
    "for i, authors_list in tqdm.tqdm(enumerate(all_papers_copy[\"authors\"].apply(ast.literal_eval))):\n",
    "    new_list = []\n",
    "    for author in authors_list:\n",
    "        if author in co_authors_df[\"id\"].values:\n",
    "            new_list.append(author)\n",
    "            \n",
    "    all_papers_copy.at[i, \"authors\"] = new_list\n",
    "\n",
    "\n",
    "# Save papers to csv\n",
    "all_papers_copy.to_csv(\"data/THE_PAPER_DATASET.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dataset summary: How many works are listed in your IC2S2 papers dataframe?\n",
    "* 9615\n",
    "\n",
    "How many unique researchers have co-authored these works?\n",
    "* 12766\n",
    "\n",
    "Efficiency in code: Describe the strategies you implemented to make your code more efficient. How did your approach affect your code's execution time?\n",
    "* As many search filters as possible were moved to the filter-parameter, as this resulted in fewer results to loop through.\n",
    "* Additionally, the searches were done in batches of names (size=25), paging through the results (200 per page), thereby avoiding many non-full pages. This sped up the run time a lot.\n",
    "* Parallel processing was not implemented due to lack of time. This would have sped up the runtime drastically, but was mostly relevant in the functions not included in this script.\n",
    "* When creating the author table, if a name-search didn't contain all desired columns, it was dropped. This results in fewer names than desired but avoids a dataset with missing values. During the same process, if multiple results for a name was found, the first exact match was used. This ensured that names like Adam and Alan weren't interchanged, even though both showed up in results.\n",
    "\n",
    "\n",
    "Filtering Criteria and Dataset Relevance: Reflect on the rationale behind setting specific thresholds for the total number of works by an author, the citation count, the number of authors per work, and the relevance of works to specific fields. How do these filtering criteria contribute to the relevance of the dataset you compiled? \n",
    "* Filtering of field semantics attempts to get articles within our desired scientific field.\n",
    "* Citation thresholding to >10 citations avoids articles that aren't well-recognized within the field.\n",
    "* Author thresholds on >5 works ensures that relevant and active authors are selected. Limiting to authors with <5000 articles attempts to limit to relevant authors within our desired field. Many articles increases the likelihood that the author is working on a very wide range of or in interdisciplinary fields, which could be an irrelevant author.\n",
    "* Thresholding to <10 authors per work also attempts to avoid very broad and non-specific (irrelevant) articles.\n",
    "\n",
    "\n",
    "Do you believe any aspects of Computational Social Science research might be underrepresented or overrepresented as a result of these choices?\n",
    "* On one hand, the thresholding could lead to the exclusion of too many authors, because they don't satisfy our thresholds.\n",
    "* But we believe that there is a greater amount of included irrelevant authors, eg. based on the semantic filters. For example, a random physics article with political importance would be included because of the semantic filters. This would be irrelevant to our search, and there could be many other examples."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 4: The Network of Computational Social Scientists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import tqdm\n",
    "import ast\n",
    "import itertools\n",
    "from statistics import median, mode\n",
    "import json\n",
    "import numpy as np\n",
    "\n",
    "# Load papers csv-file\n",
    "papers = pd.read_csv('data/THE_PAPER_DATASET.csv')\n",
    "authors = pd.read_csv('data/THE_AUTHOR_DATASET.csv')\n",
    "\n",
    "# Weird CSV\n",
    "papers['authors'] = papers['authors'].apply(ast.literal_eval)\n",
    "\n",
    "# From the dataframe pull the authors\n",
    "co_authors = papers['authors']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Part 1: Network Construction:\n",
    "\n",
    "#### 1.  Weighted Edgelist Creation: \n",
    "Start with your dataframe of papers. Construct a weighted edgelist where each list element is a tuple containing three elements: the author ids of two collaborating authors and the total number of papers they've co-authored. Ensure each author pair is listed only once.\n",
    "\n",
    "\n",
    "#### 2.  Graph Construction: \n",
    "- Use NetworkX to create an undirected Graph.\n",
    "- Employ the add_weighted_edges_from function to populate the graph with the weighted edgelist from step 1, creating a weighted, undirected graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "GRR = nx.Graph()\n",
    "\n",
    "# Dictionary to store the count of collaborations between author pairs\n",
    "collaboration_count = {}\n",
    "\n",
    "for author_list in co_authors:\n",
    "    author_list = sorted(author_list)\n",
    "    \n",
    "    if len(author_list) == 1:\n",
    "        GRR.add_node(author_list[0])\n",
    "        \n",
    "    for author_pair in itertools.combinations(author_list,2):\n",
    "        if author_pair in collaboration_count:\n",
    "            collaboration_count[author_pair] += 1\n",
    "        else:\n",
    "            collaboration_count[author_pair] = 1\n",
    "            \n",
    "edge_list = []\n",
    "for (author1, author2), count in collaboration_count.items():\n",
    "    edge_list.append((author1, author2, count))\n",
    "    \n",
    "GRR.add_weighted_edges_from(edge_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Graph with 12578 nodes and 47712 edges\n"
     ]
    }
   ],
   "source": [
    "print(GRR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "#### 3. Node Attributes:\n",
    "\n",
    "* For each node, add attributes for the author's display name, country, citation count, and the year of their first publication in Computational Social Science. The display name and country can be retrieved from your authors dataset. The year of their first publication and the citation count can be retrieved from the papers dataset.\n",
    "* Save the network as a JSON file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/12578 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 12578/12578 [07:02<00:00, 29.75it/s]\n"
     ]
    }
   ],
   "source": [
    "# Add attributes for each node\n",
    "for node in tqdm.tqdm(GRR.nodes()):\n",
    "    \n",
    "    # Filter papers for the current author\n",
    "    author_papers = papers[papers['authors'].apply(lambda x: node in x)] # find papers that the author has contributed to\n",
    "\n",
    "    # Check if the author has any publications\n",
    "    if not author_papers.empty:\n",
    "        first_publication_year = author_papers['publication_year'].min()\n",
    "        total_citation_count = author_papers['cited_by_count'].sum()\n",
    "    else:\n",
    "        first_publication_year = 0\n",
    "        total_citation_count = 0\n",
    "    \n",
    "    display_name = authors[authors['id'] == node][\"display_name\"].values[0]\n",
    "    country = authors[authors['id'] == node][\"country_code\"].values[0]\n",
    "   \n",
    "    # Add node attributes    \n",
    "    GRR.nodes[node][\"display_name\"] = display_name\n",
    "    GRR.nodes[node][\"country\"] = country\n",
    "    GRR.nodes[node][\"first_publication_year\"] = str(first_publication_year)\n",
    "    GRR.nodes[node][\"citation_count\"] = str(total_citation_count)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Converting the network to a JSON file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Convert the network to a JSON file\n",
    "graph_data = nx.json_graph.node_link_data(GRR)\n",
    "\n",
    "# Save the JSON data to a file\n",
    "with open(\"data/graph_data.json\", \"w\") as json_file:\n",
    "    json.dump(graph_data, json_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Part 2: Preliminary Network Analysis "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Graph with 12578 nodes and 47712 edges\n",
      "Density: 0.00060321\n",
      "The graph is disconnected.\n",
      "The number of connected components: 87\n",
      "The number of isolated nodes: 10\n"
     ]
    }
   ],
   "source": [
    "# total number of nodes and links in the network:\n",
    "print(GRR)\n",
    "\n",
    "graph_density = nx.density(GRR)\n",
    "print(f\"Density: {graph_density:.8f}\")\n",
    "\n",
    "# Check if the graph is connected\n",
    "is_fully_connected = nx.is_connected(GRR)\n",
    "\n",
    "if is_fully_connected:\n",
    "    print(\"The graph is fully connected.\")\n",
    "else:\n",
    "    print(\"The graph is disconnected.\")\n",
    "\n",
    "# Find connected components\n",
    "connected_components = list(nx.connected_components(GRR))\n",
    "print(\"The number of connected components:\",len(connected_components))\n",
    "\n",
    "# Find isolated nodes\n",
    "isolated_nodes = list(nx.isolates(GRR))\n",
    "print(\"The number of isolated nodes:\",len(isolated_nodes))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Network Metrics\n",
    "What is the total number of nodes (authors) and links (collaborations) in the network?\n",
    "* The total number of nodes is 12578 with 47712 links.\n",
    "\n",
    "Calculate the network's density (the ratio of actual links to the maximum possible number of links). Would you say that the network is sparse? Justify your answer.\n",
    "* The network's density is 0.00060321. The network is sparse, as only 0.6% of the possible number of links are present in the network.\n",
    "\n",
    "Is the network fully connected (i.e., is there a direct or indirect path between every pair of nodes within the network), or is it disconnected?\n",
    "* The network is disconnected, as there are isolated nodes in the network.\n",
    "\n",
    "If the network is disconnected, how many connected components does it have? A connected component is defined as a subset of nodes within the network where a path exists between any pair of nodes in that subset.\n",
    "* There are 87 connected components in the network. \n",
    "\n",
    "How many isolated nodes are there in your network? An isolated node is defined as a node with no connections to any other node in the network.\n",
    "* There are 10 isolated nodes in the network. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Discuss the results above on network density, and connectivity. Are your findings in line with what you expected? Why? (answer in max 150 words)\n",
    "* The low network density is expected, since it is unrealistic that every author has collaborated with every other author. Another reason for this is that the authors may work in completely different fields and because the scientists are in different social/professional groups. \n",
    "* The large number of connected components is not a surprise, as social networks tend to cluster as seen in Centola's experiment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "#### 2. Degree Analysis:\n",
    "- Compute the average, median, mode, minimum, and maximum degree of the nodes. Perform the same analysis for node strength (weighted degree)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Degree Analysis:\n",
      "Average Degree: 7.586579742407378\n",
      "Median Degree: 6.0\n",
      "Mode Degree: 4\n",
      "Minimum Degree: 0\n",
      "Maximum Degree: 166\n",
      "\n",
      "Weighted Degree Analysis:\n",
      "Average Weighted Degree: 13.498966449356018\n",
      "Median Weighted Degree: 7.0\n",
      "Mode Weighted Degree: 4\n",
      "Minimum Weighted Degree: 0\n",
      "Maximum Weighted Degree: 344\n"
     ]
    }
   ],
   "source": [
    "# Compute node degrees\n",
    "degrees = dict(GRR.degree())\n",
    "weighted_degrees = dict(GRR.degree(weight='weight'))\n",
    "\n",
    "# Compute the specificied values\n",
    "average_degree = np.mean(list(degrees.values()))\n",
    "median_degree = median(list(degrees.values()))\n",
    "try:\n",
    "    mode_degree = mode(list(degrees.values())) #Degree value that occurs with highest frequency among the nodes\n",
    "except:\n",
    "    mode_degree = \"No unique mode\"\n",
    "min_degree = min(degrees.values())\n",
    "max_degree = max(degrees.values())\n",
    "\n",
    "# Same calculations but for STRENGTH (WEIGHTED DEGREE)\n",
    "average_weighted_degree = np.mean(list(weighted_degrees.values()))\n",
    "median_weighted_degree = median(list(weighted_degrees.values()))\n",
    "try:\n",
    "    mode_weighted_degree = mode(list(weighted_degrees.values()))\n",
    "except:\n",
    "    mode_weighted_degree = \"No unique mode\"\n",
    "min_weighted_degree = min(weighted_degrees.values())\n",
    "max_weighted_degree = max(weighted_degrees.values())\n",
    "\n",
    "print(\"Degree Analysis:\")\n",
    "print(f\"Average Degree: {average_degree}\")\n",
    "print(f\"Median Degree: {median_degree}\")\n",
    "print(f\"Mode Degree: {mode_degree}\")\n",
    "print(f\"Minimum Degree: {min_degree}\")\n",
    "print(f\"Maximum Degree: {max_degree}\")\n",
    "\n",
    "print(\"\\nWeighted Degree Analysis:\")\n",
    "print(f\"Average Weighted Degree: {average_weighted_degree}\")\n",
    "print(f\"Median Weighted Degree: {median_weighted_degree}\")\n",
    "print(f\"Mode Weighted Degree: {mode_weighted_degree}\")\n",
    "print(f\"Minimum Weighted Degree: {min_weighted_degree}\")\n",
    "print(f\"Maximum Weighted Degree: {max_weighted_degree}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "What do these metrics tell us about the network? (answer in max 150 words)\n",
    "* On average the scientists have collaborated with almost 8 other unique scientists, but most have only collaborated with 4 others, as seen on the mode degree. \n",
    "* The average degree being larger than both the median and mode degree suggests that a few scientists have significantly more unique collaborators as seen in the maximum degree of 166 (The degree distribution is right-skewed).\n",
    "* Taking the repeated collaborations (weights) into account, the average and maximum degree has increased and the patterns mentioned above are enhanced (More right-skewed distribution). As the mode degree remains 4, the increased average and maximum degree suggests that a few scientists collaborate with many scientists and that they collaborate multiple times with them. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "#### 3. Top Authors\n",
    "- Identify the top 5 authors by degree. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 5 Authors by Degree:\n",
      "Author-ID: https://openalex.org/A5075080019\n",
      "Name: Qin Li\n",
      "Degree: 166\n",
      "Author-ID: https://openalex.org/A5005493160\n",
      "Name: Mariano Sigman\n",
      "Degree: 151\n",
      "Author-ID: https://openalex.org/A5017914184\n",
      "Name: Stephan Lewandowsky\n",
      "Degree: 145\n",
      "Author-ID: https://openalex.org/A5029100305\n",
      "Name: Denny Borsboom\n",
      "Degree: 140\n",
      "Author-ID: https://openalex.org/A5055710645\n",
      "Name: Jon Kleinberg\n",
      "Degree: 127\n"
     ]
    }
   ],
   "source": [
    "# Identify the top 5 authors by degree\n",
    "top_authors = sorted(degrees.items(), key=lambda x: x[1], reverse=True)[:5]\n",
    "\n",
    "# Print the top authors and their degrees\n",
    "print(\"Top 5 Authors by Degree:\")\n",
    "for author, degree in top_authors:\n",
    "    name = authors[authors['id'] == author]['display_name']\n",
    "    print(f\"Author-ID: {author}\")\n",
    "    print(f\"Name: {name.values[0]}\")\n",
    "    print(f\"Degree: {degree}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What role do these node play in the network?\n",
    "* These are the authors who have collaborated with the most people. It could be that the authors have either published many works or that their published works have many authors (but 10 or less). But it is probably a mix of both. These top authors are likely to be renowned and valuable in their fields."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Research these authors online. What areas do they specialize in? Do you think that their work aligns with the themes of Computational Social Science? If not, what could be possible reasons? (answer in max 150 words)\n",
    "\n",
    "Qin Li:\n",
    "* Biology/Chemistry\n",
    "* Environmental science\n",
    "\n",
    "Mariano Sigman:\n",
    "* Cognitive/Neuro science\n",
    "\n",
    "Stephan Lewandowsky\n",
    "* Psychology\n",
    "* Environmental science\n",
    "\n",
    "Denny Borsboom\n",
    "* Psychology\n",
    "* Computer science\n",
    "\n",
    "Jon Kleinberg\n",
    "* Information science\n",
    "* Social science\n",
    "\n",
    "\n",
    "As seen above, most of the authors have specialized fields that align well with computational social science. However, the two most collaborating authors (Li and Sigman) seem to be very specialized within fields that deviates slightly from computational social science. This may be because they are used in works for expert knowledge, since their fields are useful for social science research. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "CompSS",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
