{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Link to git repository: https://github.com/ongiboy/Cognitive-Social-Science\n",
    "\n",
    "Group member's contribution:\n",
    "* Christian Ong (s204109)   : 33%\n",
    "* Daniel Ries (s21)         : 33%\n",
    "* Kavus Latifi (s21)        : 33%"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 1: Web-scraping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "# Setup the URL\n",
    "LINK = \"https://ic2s2-2023.org/program\"\n",
    "r = requests.get(LINK)\n",
    "soup = BeautifulSoup(r.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "table = soup.find(\"table\",{\"class\":\"tutorials\"})\n",
    "table_rows = table.find_all(\"tr\")\n",
    "\n",
    "# For each row find the elements that include \"Keynote\" and save to list\n",
    "keynotes = []\n",
    "for tr in table_rows:\n",
    "    td = tr.find_all(\"td\")\n",
    "    row = [i.text for i in td]\n",
    "    if row:\n",
    "        if \"Keynote\" in row[1]:\n",
    "            keynotes.append(row[1])\n",
    "\n",
    "print(keynotes)\n",
    "\n",
    "# Remove \"Keynote - \" from each element in list\n",
    "keynotes = [re.sub(\"Keynote - \", \"\", i) for i in keynotes]\n",
    "\n",
    "print(keynotes)\n",
    "print(\"Number of keynotes: \", len(list(set(keynotes))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chairs = []\n",
    "# Find the sections with names\n",
    "sections = soup.find_all(\"h2\")\n",
    "for section in sections:\n",
    "\n",
    "    # Find names within section\n",
    "    bullets = section.find_all(\"i\")\n",
    "    for nameline in bullets:\n",
    "\n",
    "        # Add only names after \"Chairs\" to list (Chair: Taha Yasseri)\n",
    "        if \"Chair\" in nameline.text:\n",
    "            chairs.append(nameline.text[7:])\n",
    "        \n",
    "# unique elements in names\n",
    "print(chairs)\n",
    "print(\"Number of chairs: \", len(list(set(chairs))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "names = []\n",
    "# Find the sections with names\n",
    "sections = soup.find_all(\"ul\",{\"class\":\"nav_list\"})\n",
    "for section in sections:\n",
    "\n",
    "    # Find names within section\n",
    "    bullets = section.find_all(\"i\")\n",
    "    for nameline in bullets:\n",
    "\n",
    "        # Split into each name and add to list\n",
    "        names.extend(nameline.text.split(\", \"))\n",
    "        \n",
    "print(names)\n",
    "print(\"Number of names: \", len(list(set(names))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine all lists into one\n",
    "all_names = keynotes + chairs + names\n",
    "\n",
    "# Remove duplicates\n",
    "all_names = list(set(all_names))\n",
    "\n",
    "print(\"Number of names: \", len(all_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save names in csv\n",
    "df = pd.DataFrame(all_names, columns=[\"name\"])\n",
    "df.to_csv(\"names.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How many unique researchers do you get?\n",
    "* 1491\n",
    "\n",
    "Explain the process you followed to web-scrape the page. \n",
    "Which choices did you make to accurately retreive as many names as possible? Which strategies did you use to assess the quality of your final list?\n",
    "* First, a detailed inspection of the page was made. The sections containing names were found. The sections containing names are:\n",
    "    * An overview table containing keynote speakers (< td >)\n",
    "    * Plenary talks containing chairs (< h2 >) and speakers (< i >)\n",
    "    * Parallel talks containing chairs (< h2 >) and speakers (< i >)\n",
    "    * Posters (< i >)\n",
    "* These sections were scraped for names and combined in a list.\n",
    "* Then, the list was made to a set to remove potential duplicates."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2: Ready Made vs Custom Made Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What are pros and cons of the custom-made data used in Centola's experiment (the first study presented in the lecture) and the ready-made data used in Nicolaides's study (the second study presented in the lecture)?\n",
    "* Centola's data samples and experimental setup were highly controlled, so the causal effect of clustered vs random networks on spread behavior could be investigated while avoiding confounding effects effectively. However, ethical aspects of transparency of the experimental data could be discussed, as the people registering to the health website were not informed about their participation of the experiment. The controlled set-up might also create an artificial environment not accurately reflecting clustered networks of the real world.  \n",
    "\n",
    "* The observational data from the fitness tracking app combined with the social network app used in Nicolaide's study, provided large samples under real world conditions, reflecting the users actual behaviors in their natural environments. However as control of data collecting were limited, potential biases in the app's user base can affect the generalizability of the findings. Other external variables may also affect the found correlation, such as the motivation or other explanations as homophily.  \n",
    "  \n",
    "\n",
    "How do you think these differences can influence the interpretation of the results in each study?\n",
    "* The findings of Centola's experiment are likely more internally valid because of the controlled setup which helps isolating the variables of interest and reduce external factors. This difference makes for a more clear understanding of the causal relationship between network structures and spread behavior. However, because of the artificial environment and lack of transparency, the results of Centolaâ€™s study may have limited external validity.\n",
    "* On the other side, the results of Nicolaide's study are more applicable to real-world scenarios, dealing with the complexity of actual social networks. However, because of the lack of control over the data collection there might be potential biases and confounding variables, making it necessary to interpret the findings with caution.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 3: Gathering Research Articles using the OpenAlex API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "import tqdm\n",
    "import unicodedata\n",
    "\n",
    "def normalize_name(name):\n",
    "    normalized_name = ''.join(c for c in unicodedata.normalize('NFD', name) if unicodedata.category(c) != 'Mn' and (c.isalnum() or c.isspace()))\n",
    "    return normalized_name.lower()\n",
    "\n",
    "\n",
    "def get_info_from_names(df_names):\n",
    "\n",
    "    BASE_URL = 'https://api.openalex.org'\n",
    "    RESOURCE = '/authors'\n",
    "\n",
    "    data = []\n",
    "    for i in tqdm.tqdm(range(len(df_names))):\n",
    "        try:\n",
    "            response = requests.get(BASE_URL + RESOURCE, params={\"search\": df_names[\"name\"][i]})\n",
    "            all_results = response.json()[\"results\"]#[0]\n",
    "\n",
    "            names = []\n",
    "            normalized_df_name = normalize_name(df_names[\"name\"][i])\n",
    "\n",
    "            for r in range(len(all_results)):\n",
    "                name = all_results[r][\"display_name\"]\n",
    "                names.append(normalize_name(name))\n",
    "\n",
    "                if normalized_df_name in names:\n",
    "                    results = response.json()[\"results\"][r]\n",
    "                    break\n",
    "\n",
    "            if normalized_df_name not in names:\n",
    "                continue\n",
    "\n",
    "            data += [[results[\"id\"], results[\"display_name\"], results[\"works_api_url\"], results[\"summary_stats\"][\"h_index\"], results[\"works_count\"], results[\"last_known_institution\"][\"country_code\"]]]\n",
    "\n",
    "        except (IndexError, KeyError, ValueError, TypeError) as e:\n",
    "            print(f\"Skipping data point at iteration {i} due to error: {e}\")\n",
    "            continue  # Skip the rest of the loop and proceed to the next iteration\n",
    "        except requests.exceptions.RequestException as e:\n",
    "            print(f\"Request error at iteration {i}: {e}\")\n",
    "            continue\n",
    "\n",
    "    new_df = pd.DataFrame(data, columns=[\"id\",\"display_name\",\"works_api_url\",\"h_index\",\"works_count\", \"country_code\"])\n",
    "    return new_df\n",
    "\n",
    "\n",
    "def get_info_from_names_old(names):\n",
    "    \"\"\"\n",
    "    names --> author table (id, name, works_api_url, h_index, works_count, country_code)\n",
    "    \"\"\"\n",
    "\n",
    "    BASE_URL = 'https://api.openalex.org'\n",
    "    RESOURCE = '/authors'\n",
    "    people_info = []\n",
    "    for i,name in tqdm.tqdm(enumerate(names['name'])):\n",
    "  \n",
    "        # search for person\n",
    "        person = requests.get(BASE_URL + RESOURCE, params={'search': name}).json()\n",
    "\n",
    "        # No results\n",
    "        if len(person['results']) == 0:\n",
    "            continue\n",
    "\n",
    "        # take first result\n",
    "        person = person['results'][0]\n",
    "\n",
    "        # if person doesnt have all info, skip person\n",
    "        try:\n",
    "            # extract desired information\n",
    "            person_info = [person['id'], person['display_name'], person['works_api_url'], person['summary_stats']['h_index'], person['works_count'], person['last_known_institution']['country_code']]\n",
    "        \n",
    "        except:\n",
    "            continue\n",
    "\n",
    "        people_info.append(person_info)\n",
    "\n",
    "    people_pd = pd.DataFrame(people_info, columns=['id', 'name', 'works_api_url', 'h_index', 'works_count', 'country_code'])\n",
    "    return people_pd\n",
    "\n",
    "\n",
    "def get_concept_ids(concept_requirements):\n",
    "    \"\"\"\n",
    "    concepts --> concept_ids\n",
    "    \"\"\"\n",
    "\n",
    "    BASE_URL = 'https://api.openalex.org'\n",
    "    RESOURCE = '/concepts'\n",
    "\n",
    "    concept_ids = []\n",
    "    for concept in concept_requirements:\n",
    "        result = requests.get(BASE_URL + RESOURCE, params={'search': concept, 'filter': 'level:0'}).json()\n",
    "        concept_ids.append(result['results'][0]['id'])\n",
    "\n",
    "    concept_ids = [id.split(\"/\")[-1] for id in concept_ids]\n",
    "    return concept_ids\n",
    "\n",
    "\n",
    "def get_articles_from_authors(names, concept_ids_requirements_1, concept_ids_requirements_2, subset=False):\n",
    "    \"\"\"\n",
    "    Extracts articles from authors in the names table.\n",
    "        The articles are filtered by the criteria from the assignment description.\n",
    "    \"\"\"\n",
    "    \n",
    "    BASE_URL = 'https://api.openalex.org'\n",
    "    RESOURCE = '/works'\n",
    "\n",
    "    # Filter out authors not having 5-5000 works\n",
    "    names = names[(names['works_count']>=5) & (names['works_count']<=5000)]\n",
    "\n",
    "    table1 = []\n",
    "    table2 = []\n",
    "\n",
    "    # Search for articles in batches of 25 authors\n",
    "    name_batches = [list(names['id'][i:i+25]) for i in range(0, len(names), 25)]\n",
    "\n",
    "    for num_name_batch, name_batch in enumerate(name_batches):\n",
    "\n",
    "        # track progress\n",
    "        print(\"batch number:\", num_name_batch)\n",
    "\n",
    "        # short version for testing\n",
    "        if subset and num_name_batch>0:\n",
    "            break\n",
    "\n",
    "        # Scroll through the results\n",
    "        cursor = '*'\n",
    "        while True:\n",
    "            filters = ['cited_by_count:>10', \n",
    "                        'authors_count:<10',\n",
    "                        'authorships.author.id:'+'|'.join(name_batch),\n",
    "                        'concepts.id:'+'|'.join(concept_ids_requirements_1),\n",
    "                        'concepts.id:'+'|'.join(concept_ids_requirements_2)\n",
    "                        ]\n",
    "            parameters = {'per-page': 200,\n",
    "                            'filter': ','.join(filters),\n",
    "                            'cursor': cursor\n",
    "                            }\n",
    "            result = requests.get(BASE_URL + RESOURCE, params=parameters).json()\n",
    "\n",
    "            # If last page is reached (which is empty), break \n",
    "            cursor = result['meta']['next_cursor'] # next page for next search\n",
    "            if len(result['results'])==0 or cursor is None:\n",
    "                break\n",
    "\n",
    "            # Go through all articles and extract information\n",
    "            for n_article,article in enumerate(result['results']):\n",
    "                try:\n",
    "                    tab1 = [article['id'], article['publication_year'], article['cited_by_count'], [author['author']['id'] for author in article['authorships']]]\n",
    "                    tab2 = [article['id'], article['title'], article['abstract_inverted_index']]\n",
    "                    table1.append(tab1)\n",
    "                    table2.append(tab2)\n",
    "\n",
    "                except:\n",
    "                    print(\"skipped name batch:\", num_name_batch, \"article:\", n_article)\n",
    "                    continue\n",
    "    \n",
    "    table1 = pd.DataFrame(table1, columns=['id', 'publication_year', 'cited_by_count', 'authors'])\n",
    "    table2 = pd.DataFrame(table2, columns=['id', 'title', 'abstract_inverted_index'])\n",
    "\n",
    "    return table1, table2\n",
    "\n",
    "\n",
    "def get_info_from_author_ids(authors):\n",
    "    \"\"\"\n",
    "    Gets info table from author ids.\n",
    "    \"\"\"\n",
    "\n",
    "    URL = 'https://api.openalex.org/authors'\n",
    "\n",
    "    co_author_info = []\n",
    "    for i,author_id in enumerate(authors):\n",
    "        # track progress\n",
    "        if i!=0 and i%50==0:\n",
    "            print(i)\n",
    "            \n",
    "        # search for person\n",
    "        author_id = author_id.split('/')[-1]\n",
    "        person = requests.get(URL + '/' + author_id).json()\n",
    "\n",
    "        # No results\n",
    "        if len(person) == 0:\n",
    "            continue\n",
    "\n",
    "        # if person doesnt have all info, skip person\n",
    "        try:\n",
    "            # extract desired information\n",
    "            person_info = [author_id, person['display_name'], person['works_api_url'], person['summary_stats']['h_index'], person['works_count'], person['last_known_institution']['country_code']]\n",
    "        \n",
    "        except:\n",
    "            continue\n",
    "\n",
    "        co_author_info.append(person_info)\n",
    "\n",
    "    return co_author_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, turn names-list from Part 1 into a pandas dataframe\n",
    "names = pd.read_csv('data/names.csv')\n",
    "\n",
    "# Get info from names (table of authors + info)\n",
    "authors = get_info_from_names(names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save authors to csv\n",
    "authors.to_csv(\"authors.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get concept ids\n",
    "concepts_requirements_1 = ['Sociology', 'Psychology', 'Economics', 'Political Science']\n",
    "concepts_requirements_2 = ['Mathematics', 'Physics', 'Computer Science']\n",
    "concept_ids_1 = get_concept_ids(concepts_requirements_1)\n",
    "concept_ids_2 = get_concept_ids(concepts_requirements_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get articles made by names\n",
    "authors = pd.read_csv('data/authors_final.csv')\n",
    "\n",
    "articles_from_authors, abstracts_from_authors = get_articles_from_authors(authors, concept_ids_1, concept_ids_2, subset=False)\n",
    "print(\"Number of articles:\", len(articles_from_authors))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save articles to csv\n",
    "articles_from_authors.to_csv(\"papers.csv\", index=False)\n",
    "abstracts_from_authors.to_csv(\"abstracts.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get unique authors from articles\n",
    "authors_unique = articles_from_authors['authors'].explode().dropna().unique()\n",
    "print(\"Number of unique authors:\", len(authors_unique))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dataset summary: How many works are listed in your IC2S2 papers dataframe?\n",
    "* 9609\n",
    "\n",
    "How many unique researchers have co-authored these works?\n",
    "* 12766\n",
    "\n",
    "Efficiency in code: Describe the strategies you implemented to make your code more efficient. How did your approach affect your code's execution time?\n",
    "* As many of the search filters as possible were moved to the filter parameter, as this resulted in fewer results to filter through.\n",
    "* Additionally, the search was done in batches of names, paging through the results (200 per page) and thereby avoiding many pages with few results.\n",
    "* A major weakness is the concept filter, which is hard coded instead of using the 'concepts.id'-parameter due to lack of time. Implementing this would significantly improve the efficiency and would only require a list of the concept-OpenAlex-IDs. Also, multiprocessing isn't implemented despite the suggestion (also due to lack of time)\n",
    "* The names searched for was based on the author table. If a name search didn't contain all desired columns (eg. country_code), the name was dropped. This results in fewer names than desired but avoids a dataset with missing values.\n",
    "\n",
    "Filtering Criteria and Dataset Relevance: Reflect on the rationale behind setting specific thresholds for the total number of works by an author, the citation count, the number of authors per work, and the relevance of works to specific fields. How do these filtering criteria contribute to the relevance of the dataset you compiled? \n",
    "* Filtering of field semantics ensures articles within out desired scientific field.\n",
    "* Citation thresholding to >10 citations ensures well-acknowledged articles.\n",
    "* Author thresholds on >5 works ensures that relevant and active authors are selected. Limiting to authors with <5000 articles attempts to limit to relevant authors within our desired field. Many articles increases the likelihood that the author is working on a very wide range of or in interdisciplinary fields, which could be an irrelevant author.\n",
    "* Thresholding to <10 authors per work also attempts to avoid very broad and non-specific (irrelevant) articles.\n",
    "\n",
    "Do you believe any aspects of Computational Social Science research might be underrepresented or overrepresented as a result of these choices?\n",
    "* On one hand, the thresholding could lead to the exclusion of too many authors, because they don't satisfy our thresholds.\n",
    "* But we believe that there is a greater amount of included irrelevant authors, eg. based on the semantic filters. For example, a random physics article with political importance would be included because of the semantic filters. This would be irrelevant to our search, and there are many other examples."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 4: The Network of Computational Social Scientists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import tqdm\n",
    "import ast\n",
    "import itertools\n",
    "\n",
    "# Load papers csv-file\n",
    "papers = pd.read_csv('data/papers.csv')\n",
    "authors_final = pd.read_csv('data/authors_final.csv')\n",
    "\n",
    "# Weird CSV\n",
    "papers['authors'] = papers['authors'].apply(ast.literal_eval)\n",
    "\n",
    "# From the dataframe pull the authors\n",
    "co_authors = papers['authors']"
   ]
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "GRR = nx.Graph()\n",
    "\n",
    "# Dictionary to store the count of collaborations between author pairs\n",
    "collaboration_count = {}\n",
    "\n",
    "for author_list in co_authors:\n",
    "    author_list = sorted(author_list)\n",
    "    \n",
    "    for author_pair in itertools.combinations(author_list,2):\n",
    "        if author_pair in collaboration_count:\n",
    "            collaboration_count[author_pair] += 1\n",
    "        else:\n",
    "            collaboration_count[author_pair] = 1\n",
    "            \n",
    "edge_list = []\n",
    "for (author1, author2), count in collaboration_count.items():\n",
    "    edge_list.append((author1, author2, count))\n",
    "    \n",
    "GRR.add_weighted_edges_from(edge_list)"
   ],
   "metadata": {
    "collapsed": false
   },
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "CompSS",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
