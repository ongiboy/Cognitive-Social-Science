{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Link to git repository: https://github.com/ongiboy/Cognitive-Social-Science\n",
    "\n",
    "Group member's contribution:\n",
    "* Every task was made in collaboration by all members."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 1: Web-scraping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "# Setup the URL\n",
    "LINK = \"https://ic2s2-2023.org/program\"\n",
    "r = requests.get(LINK)\n",
    "soup = BeautifulSoup(r.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extracting keynote speakers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "table = soup.find(\"table\",{\"class\":\"tutorials\"})\n",
    "table_rows = table.find_all(\"tr\")\n",
    "\n",
    "# For each row find the elements that include \"Keynote\" and save to list\n",
    "keynotes = []\n",
    "for tr in table_rows:\n",
    "    td = tr.find_all(\"td\")\n",
    "    row = [i.text for i in td]\n",
    "    if row:\n",
    "        if \"Keynote\" in row[1]:\n",
    "            keynotes.append(row[1])\n",
    "\n",
    "# Remove \"Keynote - \" from each element in list\n",
    "keynotes = [re.sub(\"Keynote - \", \"\", i) for i in keynotes]\n",
    "\n",
    "print(keynotes)\n",
    "print(\"Number of keynotes: \", len(list(set(keynotes))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extracting chairs people"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chairs = []\n",
    "# Find the sections with names\n",
    "sections = soup.find_all(\"h2\")\n",
    "for section in sections:\n",
    "\n",
    "    # Find names within section\n",
    "    bullets = section.find_all(\"i\")\n",
    "    for nameline in bullets:\n",
    "\n",
    "        # Add only names after \"Chairs\" to list (Chair: Taha Yasseri)\n",
    "        if \"Chair\" in nameline.text:\n",
    "            chairs.append(nameline.text[7:])\n",
    "        \n",
    "# unique elements in names\n",
    "print(chairs)\n",
    "print(\"Number of chairs: \", len(list(set(chairs))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extracting speakers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "names = []\n",
    "# Find the sections with names\n",
    "sections = soup.find_all(\"ul\",{\"class\":\"nav_list\"})\n",
    "for section in sections:\n",
    "\n",
    "    # Find names within section\n",
    "    bullets = section.find_all(\"i\")\n",
    "    for nameline in bullets:\n",
    "\n",
    "        # Split into each name and add to list\n",
    "        names.extend(nameline.text.split(\", \"))\n",
    "        \n",
    "print(names)\n",
    "print(\"Number of names: \", len(list(set(names))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Combined list of all researchers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine all lists into one\n",
    "all_names = keynotes + chairs + names\n",
    "\n",
    "# Remove duplicates\n",
    "all_names = list(set(all_names))\n",
    "\n",
    "print(\"Number of names: \", len(all_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save names in csv\n",
    "df = pd.DataFrame(all_names, columns=[\"name\"])\n",
    "df.to_csv(\"names.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How many unique researchers do you get?\n",
    "* 1491\n",
    "\n",
    "Explain the process you followed to web-scrape the page. \n",
    "Which choices did you make to accurately retreive as many names as possible? Which strategies did you use to assess the quality of your final list?\n",
    "* First, a detailed inspection of the page was made. The sections containing names were found. These sections are:\n",
    "    * An overview table containing keynote speakers (< td >)\n",
    "    * Plenary talks containing chairs (< h2 >) and speakers (< i >)\n",
    "    * Parallel talks containing chairs (< h2 >) and speakers (< i >)\n",
    "    * Posters (< i >)\n",
    "* The sections were scraped for names. In the overview table, in each row Finally the lists combined. \n",
    "* Then, the list was made to a set to remove potential duplicates."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2: Ready Made vs Custom Made Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What are pros and cons of the custom-made data used in Centola's experiment (the first study presented in the lecture) and the ready-made data used in Nicolaides's study (the second study presented in the lecture)?\n",
    "* Centola's data samples and experimental setup were highly controlled, so the causal effect of clustered vs random networks on spread behavior could be investigated while avoiding confounding effects effectively. However, ethical aspects of transparency of the experimental data could be discussed, as the people registering to the health website were not informed about their participation of the experiment. The controlled set-up might also create an artificial environment not accurately reflecting clustered networks of the real world.  \n",
    "\n",
    "* The observational data from the fitness tracking app combined with the social network app used in Nicolaide's study, provided large samples under real world conditions, reflecting the users actual behaviors in their natural environments. However as control of data collecting were limited, potential biases in the app's user base can affect the generalizability of the findings. Other external variables may also affect the found correlation, such as the motivation or other explanations as homophily.  \n",
    "  \n",
    "\n",
    "How do you think these differences can influence the interpretation of the results in each study?\n",
    "* The findings of Centola's experiment are likely more internally valid because of the controlled setup which helps isolating the variables of interest and reduce external factors. This difference makes for a more clear understanding of the causal relationship between network structures and spread behavior. However, because of the artificial environment and lack of transparency, the results of Centolaâ€™s study may have limited external validity.\n",
    "* On the other side, the results of Nicolaide's study are more applicable to real-world scenarios, dealing with the complexity of actual social networks. However, because of the lack of control over the data collection there might be potential biases and confounding variables, making it necessary to interpret the findings with caution.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 3: Gathering Research Articles using the OpenAlex API"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define functions and import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "import tqdm\n",
    "import unicodedata\n",
    "\n",
    "def normalize_name(name):\n",
    "    \"\"\"\n",
    "    returns more standard version of name\n",
    "        (removes accents, special characters, and makes lowercase)\n",
    "    \"\"\"\n",
    "\n",
    "    normalized_name = ''.join(c for c in unicodedata.normalize('NFD', name) if unicodedata.category(c) != 'Mn' and (c.isalnum() or c.isspace()))\n",
    "    return normalized_name.lower()\n",
    "\n",
    "\n",
    "def get_info_from_names(df_names):\n",
    "    \"\"\"\n",
    "    Retrieves author information from names (through a search)\n",
    "    \"\"\"\n",
    "\n",
    "    BASE_URL = 'https://api.openalex.org'\n",
    "    RESOURCE = '/authors'\n",
    "\n",
    "    data = []\n",
    "    for i in tqdm.tqdm(range(len(df_names))):\n",
    "        try:\n",
    "            response = requests.get(BASE_URL + RESOURCE, params={\"search\": df_names[\"name\"][i]})\n",
    "            all_results = response.json()[\"results\"]#[0]\n",
    "\n",
    "            names = []\n",
    "            normalized_df_name = normalize_name(df_names[\"name\"][i])\n",
    "\n",
    "            for r in range(len(all_results)):\n",
    "                name = all_results[r][\"display_name\"]\n",
    "                names.append(normalize_name(name))\n",
    "\n",
    "                if normalized_df_name in names:\n",
    "                    results = response.json()[\"results\"][r]\n",
    "                    break\n",
    "\n",
    "            if normalized_df_name not in names:\n",
    "                continue\n",
    "\n",
    "            data += [[results[\"id\"], results[\"display_name\"], results[\"works_api_url\"], results[\"summary_stats\"][\"h_index\"], results[\"works_count\"], results[\"last_known_institution\"][\"country_code\"]]]\n",
    "\n",
    "        except (IndexError, KeyError, ValueError, TypeError) as e:\n",
    "            print(f\"Skipping data point at iteration {i} due to error: {e}\")\n",
    "            continue  # Skip the rest of the loop and proceed to the next iteration\n",
    "        except requests.exceptions.RequestException as e:\n",
    "            print(f\"Request error at iteration {i}: {e}\")\n",
    "            continue\n",
    "\n",
    "    new_df = pd.DataFrame(data, columns=[\"id\",\"display_name\",\"works_api_url\",\"h_index\",\"works_count\", \"country_code\"])\n",
    "    return new_df\n",
    "\n",
    "\n",
    "def get_concept_ids(concept_requirements):\n",
    "    \"\"\"\n",
    "    Retrieves the OpenAlex concept_ids for a list of requirements.\n",
    "    \"\"\"\n",
    "\n",
    "    BASE_URL = 'https://api.openalex.org'\n",
    "    RESOURCE = '/concepts'\n",
    "\n",
    "    concept_ids = []\n",
    "    for concept in concept_requirements:\n",
    "        result = requests.get(BASE_URL + RESOURCE, params={'search': concept, 'filter': 'level:0'}).json()\n",
    "        concept_ids.append(result['results'][0]['id'])\n",
    "\n",
    "    concept_ids = [id.split(\"/\")[-1] for id in concept_ids]\n",
    "    return concept_ids\n",
    "\n",
    "\n",
    "def get_articles_from_authors(names, concept_ids_requirements_1, concept_ids_requirements_2, subset=False):\n",
    "    \"\"\"\n",
    "    Extracts articles from authors in the names table.\n",
    "        The articles are filtered by the criteria from the assignment description.\n",
    "    \"\"\"\n",
    "    \n",
    "    BASE_URL = 'https://api.openalex.org'\n",
    "    RESOURCE = '/works'\n",
    "\n",
    "    # Filter out authors not having 5-5000 works\n",
    "    names = names[(names['works_count']>=5) & (names['works_count']<=5000)]\n",
    "\n",
    "    table1 = []\n",
    "    table2 = []\n",
    "\n",
    "    # Search for articles in batches of 25 authors\n",
    "    name_batches = [list(names['id'][i:i+25]) for i in range(0, len(names), 25)]\n",
    "\n",
    "    for num_name_batch, name_batch in tqdm.tqdm(enumerate(name_batches)):\n",
    "\n",
    "        # short version for testing\n",
    "        if subset and num_name_batch>0:\n",
    "            break\n",
    "\n",
    "        # Scroll through the results\n",
    "        cursor = '*'\n",
    "        while True:\n",
    "            filters = ['cited_by_count:>10', \n",
    "                        'authors_count:<10',\n",
    "                        'authorships.author.id:'+'|'.join(name_batch),\n",
    "                        'concepts.id:'+'|'.join(concept_ids_requirements_1),\n",
    "                        'concepts.id:'+'|'.join(concept_ids_requirements_2)\n",
    "                        ]\n",
    "            parameters = {'per-page': 200,\n",
    "                            'filter': ','.join(filters),\n",
    "                            'cursor': cursor\n",
    "                            }\n",
    "            result = requests.get(BASE_URL + RESOURCE, params=parameters).json()\n",
    "\n",
    "            # If last page is reached (which is empty), break \n",
    "            cursor = result['meta']['next_cursor'] # next page for next search\n",
    "            if len(result['results'])==0 or cursor is None:\n",
    "                break\n",
    "\n",
    "            # Go through all articles and extract information\n",
    "            for n_article,article in enumerate(result['results']):\n",
    "                try:\n",
    "                    tab1 = [article['id'], article['publication_year'], article['cited_by_count'], [author['author']['id'] for author in article['authorships']]]\n",
    "                    tab2 = [article['id'], article['title'], article['abstract_inverted_index']]\n",
    "                    table1.append(tab1)\n",
    "                    table2.append(tab2)\n",
    "\n",
    "                except:\n",
    "                    print(\"skipped name batch:\", num_name_batch, \"article:\", n_article)\n",
    "                    continue\n",
    "    \n",
    "    table1 = pd.DataFrame(table1, columns=['id', 'publication_year', 'cited_by_count', 'authors'])\n",
    "    table2 = pd.DataFrame(table2, columns=['id', 'title', 'abstract_inverted_index'])\n",
    "\n",
    "    return table1, table2\n",
    "\n",
    "\n",
    "def get_info_from_author_ids(author_ids):\n",
    "    \"\"\"\n",
    "    Gets info table from author ids.\n",
    "    \"\"\"\n",
    "\n",
    "    URL = 'https://api.openalex.org/authors'\n",
    "    author_ids = [id.split('/')[-1] for id in author_ids]\n",
    "\n",
    "    co_author_info = []\n",
    "    id_batches = [author_ids[i:i+25] for i in range(0, len(author_ids), 25)]\n",
    "\n",
    "    for i,ids_batch in tqdm.tqdm(enumerate(id_batches)):\n",
    "            \n",
    "        # search for result\n",
    "        params = {'filter': 'ids.openalex:'+'|'.join(ids_batch)}\n",
    "        result_batch = requests.get(URL, params=params).json()\n",
    "\n",
    "        # No results\n",
    "        if len(result_batch) == 0:\n",
    "            continue\n",
    "        \n",
    "        # Go through results (authors)\n",
    "        for result in result_batch['results']:\n",
    "\n",
    "            # if person doesnt have all info, skip person\n",
    "            try:\n",
    "                # extract desired information\n",
    "                person_info = [result['id'], result['display_name'], result['works_api_url'], result['summary_stats']['h_index'], result['works_count'], result['last_known_institution']['country_code']]\n",
    "            except:\n",
    "                continue\n",
    "\n",
    "            co_author_info.append(person_info)\n",
    "    \n",
    "    co_author_info_df = pd.DataFrame(co_author_info, columns=['id', 'display_name', 'works_api_url', 'h_index', 'works_count', 'country_code'])\n",
    "\n",
    "    return co_author_info_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, turn names-list from Part 1 into a pandas dataframe\n",
    "names = pd.read_csv('data/names.csv')\n",
    "\n",
    "# Get info from names (table of authors + info)\n",
    "authors = get_info_from_names(names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop duplicates\n",
    "authors = authors.drop_duplicates(subset='id')\n",
    "\n",
    "# Save authors to csv\n",
    "authors.to_csv(\"data/authors.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Authors from week 2 is downloaded\n",
    "\n",
    "(workflow in different file: \"data_preperation.ipynb\" since its is not part of the task)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "authors = pd.read_csv('data/authors_final.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, the concept_ids from the desired concepts are retrieved (for use in the search)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get concept ids\n",
    "concepts_requirements_1 = ['Sociology', 'Psychology', 'Economics', 'Political Science']\n",
    "concepts_requirements_2 = ['Mathematics', 'Physics', 'Computer Science']\n",
    "concept_ids_1 = get_concept_ids(concepts_requirements_1)\n",
    "concept_ids_2 = get_concept_ids(concepts_requirements_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, the articles by the author list are retrieved, using the specified filters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get articles made by names\n",
    "authors = pd.read_csv('data/authors_final.csv')\n",
    "\n",
    "articles_from_authors, abstracts_from_authors = get_articles_from_authors(authors, concept_ids_1, concept_ids_2, subset=False)\n",
    "print(\"Number of articles:\", len(articles_from_authors))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, duplicated are dropped and files are saved."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop duplicates from articles and abstracts\n",
    "papers = articles_from_authors.drop_duplicates(subset=['id'])\n",
    "abstracts = abstracts_from_authors.drop_duplicates(subset=['id'])\n",
    "\n",
    "# Save articles to csv\n",
    "papers.to_csv(\"data/papers.csv\", index=False)\n",
    "abstracts.to_csv(\"data/abstracts.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (OBS: Following not asked in assignment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load papers and authors\n",
    "import ast\n",
    "authors = pd.read_csv('data/authors_final.csv')\n",
    "papers = pd.read_csv('data/papers.csv')\n",
    "\n",
    "author_ids = authors[\"id\"]\n",
    "co_authors_ids = [x for x in papers[\"authors\"].copy().apply(ast.literal_eval).explode().dropna().unique() if x not in author_ids]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get info from author ids\n",
    "co_author_info = get_info_from_author_ids(co_authors_ids)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop rows in co_author_info if \"nan\" in country_code\n",
    "co_author_info = co_author_info.dropna(subset=['country_code'])\n",
    "\n",
    "# Concatenate the authors and co_author_info tables\n",
    "co_authors_df = pd.concat([authors, co_author_info]).drop_duplicates(subset=['id'])\n",
    "\n",
    "# Drop duplicates\n",
    "co_author_info = co_author_info.drop_duplicates(subset='id')\n",
    "\n",
    "# Save authors to csv\n",
    "co_author_info.to_csv(\"data/co_authors_info.csv\", index=False)\n",
    "co_authors_df.to_csv(\"data/THE_AUTHOR_DATASET.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get papers and abstracts from co-authors (We use the same concept_ids as earlier)\n",
    "papers_from_co_authors, abstracts_from_co_authors = get_articles_from_authors(co_author_info, concept_ids_1, concept_ids_2, subset=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concatenate the papers and abstracts tables\n",
    "papers_final = pd.concat([papers, papers_from_co_authors]).drop_duplicates(subset=['id'])\n",
    "abstracts_final = pd.concat([abstracts, abstracts_from_co_authors]).drop_duplicates(subset=['id'])\n",
    "\n",
    "# Save papers and abstracts to csv\n",
    "papers_final.to_csv(\"data/THE_PAPER_DATASET.csv\", index=False)\n",
    "abstracts_final.to_csv(\"data/THE_ABSTRACT_DATASET.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dataset summary: How many works are listed in your IC2S2 papers dataframe?\n",
    "* 9615\n",
    "\n",
    "How many unique researchers have co-authored these works?\n",
    "* 12766\n",
    "\n",
    "Efficiency in code: Describe the strategies you implemented to make your code more efficient. How did your approach affect your code's execution time?\n",
    "* As many of the search filters as possible were moved to the filter parameter, as this resulted in fewer results to filter (loop) through.\n",
    "* Additionally, the search was done in batches of names, paging through the results (200 per page) and thereby avoiding many non-full pages. This sped up the run time a lot.\n",
    "* Parallel processing was not implemented due to lack of time. This would have sped up the runtime drastically, but was mostly relevant in the functions not included in this script - due to a longer runtime in these functions.\n",
    "* The names searched for was based on the author table. If a name search didn't contain all desired columns (eg. country_code), the name was dropped. This results in fewer names than desired but avoids a dataset with missing values.\n",
    "* Furthermore, \n",
    "\n",
    "Filtering Criteria and Dataset Relevance: Reflect on the rationale behind setting specific thresholds for the total number of works by an author, the citation count, the number of authors per work, and the relevance of works to specific fields. How do these filtering criteria contribute to the relevance of the dataset you compiled? \n",
    "* Filtering of field semantics ensures articles within out desired scientific field.\n",
    "* Citation thresholding to >10 citations ensures well-acknowledged articles.\n",
    "* Author thresholds on >5 works ensures that relevant and active authors are selected. Limiting to authors with <5000 articles attempts to limit to relevant authors within our desired field. Many articles increases the likelihood that the author is working on a very wide range of or in interdisciplinary fields, which could be an irrelevant author.\n",
    "* Thresholding to <10 authors per work also attempts to avoid very broad and non-specific (irrelevant) articles.\n",
    "\n",
    "Do you believe any aspects of Computational Social Science research might be underrepresented or overrepresented as a result of these choices?\n",
    "* On one hand, the thresholding could lead to the exclusion of too many authors, because they don't satisfy our thresholds.\n",
    "* But we believe that there is a greater amount of included irrelevant authors, eg. based on the semantic filters. For example, a random physics article with political importance would be included because of the semantic filters. This would be irrelevant to our search, and there are many other examples."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 4: The Network of Computational Social Scientists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import tqdm\n",
    "import ast\n",
    "import itertools\n",
    "from statistics import median, mode\n",
    "import json\n",
    "import numpy as np\n",
    "\n",
    "# Load papers csv-file\n",
    "papers = pd.read_csv('data/papers.csv')\n",
    "authors_final = pd.read_csv('data/authors_final.csv')\n",
    "\n",
    "# Weird CSV\n",
    "papers['authors'] = papers['authors'].apply(ast.literal_eval)\n",
    "\n",
    "# From the dataframe pull the authors\n",
    "co_authors = papers['authors']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Part 1: Network Construction:\n",
    "\n",
    "#### 1.  Weighted Edgelist Creation: \n",
    "Start with your dataframe of papers. Construct a weighted edgelist where each list element is a tuple containing three elements: the author ids of two collaborating authors and the total number of papers they've co-authored. Ensure each author pair is listed only once.\n",
    "\n",
    "\n",
    "#### 2.  Graph Construction: \n",
    "- Use NetworkX to create an undirected Graph.\n",
    "- Employ the add_weighted_edges_from function to populate the graph with the weighted edgelist from step 1, creating a weighted, undirected graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "GRR = nx.Graph()\n",
    "\n",
    "# Dictionary to store the count of collaborations between author pairs\n",
    "collaboration_count = {}\n",
    "\n",
    "for author_list in co_authors:\n",
    "    author_list = sorted(author_list)\n",
    "    \n",
    "    for author_pair in itertools.combinations(author_list,2):\n",
    "        if author_pair in collaboration_count:\n",
    "            collaboration_count[author_pair] += 1\n",
    "        else:\n",
    "            collaboration_count[author_pair] = 1\n",
    "            \n",
    "edge_list = []\n",
    "for (author1, author2), count in collaboration_count.items():\n",
    "    edge_list.append((author1, author2, count))\n",
    "    \n",
    "GRR.add_weighted_edges_from(edge_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "#### 3. Node Attributes:\n",
    "\n",
    "* For each node, add attributes for the author's display name, country, citation count, and the year of their first publication in Computational Social Science. The display name and country can be retrieved from your authors dataset. The year of their first publication and the citation count can be retrieved from the papers dataset.\n",
    "* Save the network as a JSON file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Add attributes for each node\n",
    "for node in GRR.nodes():\n",
    "    \n",
    "    # Filter papers for the current author\n",
    "    author_papers = THE_PAPER_DATASET[THE_PAPER_DATASET['authors'].apply(lambda x: node in x)] # find papers that the author has contributed to\n",
    "\n",
    "    # Check if the author has any publications\n",
    "    if not author_papers.empty:\n",
    "        first_publication_year = author_papers['publication_year'].min()\n",
    "        total_citation_count = author_papers['cited_by_count'].sum()\n",
    "    else:\n",
    "        first_publication_year = 0\n",
    "        total_citation_count = 0\n",
    "    \n",
    "    display_name = THE_AUTHOR_DATASET[THE_AUTHOR_DATASET['id'] == node][\"display_name\"]\n",
    "    country = THE_AUTHOR_DATASET[THE_AUTHOR_DATASET['id'] == node][\"country_code\"]\n",
    "   \n",
    "    # Add node attributes    \n",
    "    GRR.nodes[node][\"display_name\"] = display_name\n",
    "    GRR.nodes[node][\"country\"] = country\n",
    "    GRR.nodes[node][\"first_publication_year\"] = first_publication_year\n",
    "    GRR.nodes[node][\"citation_count\"] = total_citation_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Convert the network to a JSON file\n",
    "graph_data = nx.json_graph.node_link_data(GRR)\n",
    "\n",
    "# Save the JSON data to a file\n",
    "with open(\"data/graph_data.json\", \"w\") as json_file:\n",
    "    json.dump(graph_data, json_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Part 2: Preliminary Network Analysis \n",
    "#### 1. Network Metrics\n",
    "- What is the total number of nodes (authors) and links (collaborations) in the network?\n",
    "- Calculate the network's density (the ratio of actual links to the maximum possible number of links). Would you say that the network is sparse? Justify your answer.\n",
    "- Is the network fully connected (i.e., is there a direct or indirect path between every pair of nodes within the network), or is it disconnected?\n",
    "- If the network is disconnected, how many connected components does it have? A connected component is defined as a subset of nodes within the network where a path exists between any pair of nodes in that subset.\n",
    "- How many isolated nodes are there in your network? An isolated node is defined as a node with no connections to any other node in the network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# total number of nodes and links in the network:\n",
    "print(GRR)\n",
    "\n",
    "graph_density = nx.density(GRR)\n",
    "print(graph_density)\n",
    "\n",
    "# Check if the graph is connected\n",
    "is_fully_connected = nx.is_connected(GRR)\n",
    "\n",
    "if is_fully_connected:\n",
    "    print(\"The graph is fully connected.\")\n",
    "else:\n",
    "    print(\"The graph is disconnected.\")\n",
    "\n",
    "# Find connected components\n",
    "connected_components = list(nx.connected_components(GRR))\n",
    "print(len(connected_components))\n",
    "\n",
    "# Find isolated nodes\n",
    "isolated_nodes = list(nx.isolates(GRR))\n",
    "print(len(isolated_nodes))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "* Discuss the results above on network density, and connectivity. Are your findings in line with what you expected? Why? (answer in max 150 words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "#### 2. Degree Analysis:\n",
    "- Compute the average, median, mode, minimum, and maximum degree of the nodes. Perform the same analysis for node strength (weighted degree)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Compute node degrees\n",
    "degrees = dict(GRR.degree())\n",
    "weighted_degrees = dict(GRR.degree(weight='weight'))\n",
    "\n",
    "# Compute average, median, mode, minimum, and maximum degree\n",
    "average_degree = np.mean(list(degrees.values()))\n",
    "median_degree = median(list(degrees.values()))\n",
    "try:\n",
    "    mode_degree = mode(list(degrees.values())) #Degree value that occurs with highest frequency among the nodes\n",
    "except:\n",
    "    mode_degree = \"No unique mode\"\n",
    "min_degree = min(degrees.values())\n",
    "max_degree = max(degrees.values())\n",
    "\n",
    "# Same calculations but for STRENGTH (WEIGHTED DEGREE)\n",
    "# Compute average, median, mode, minimum, and maximum weighted degree\n",
    "average_weighted_degree = np.mean(list(weighted_degrees.values()))\n",
    "median_weighted_degree = median(list(weighted_degrees.values()))\n",
    "try:\n",
    "    mode_weighted_degree = mode(list(weighted_degrees.values()))\n",
    "except:\n",
    "    mode_weighted_degree = \"No unique mode\"\n",
    "min_weighted_degree = min(weighted_degrees.values())\n",
    "max_weighted_degree = max(weighted_degrees.values())\n",
    "\n",
    "# Print the results\n",
    "print(\"Degree Analysis:\")\n",
    "print(f\"Average Degree: {average_degree}\")\n",
    "print(f\"Median Degree: {median_degree}\")\n",
    "print(f\"Mode Degree: {mode_degree}\")\n",
    "print(f\"Minimum Degree: {min_degree}\")\n",
    "print(f\"Maximum Degree: {max_degree}\")\n",
    "\n",
    "print(\"\\nWeighted Degree Analysis:\")\n",
    "print(f\"Average Weighted Degree: {average_weighted_degree}\")\n",
    "print(f\"Median Weighted Degree: {median_weighted_degree}\")\n",
    "print(f\"Mode Weighted Degree: {mode_weighted_degree}\")\n",
    "print(f\"Minimum Weighted Degree: {min_weighted_degree}\")\n",
    "print(f\"Maximum Weighted Degree: {max_weighted_degree}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "- What do these metrics tell us about the network? (answer in max 150 words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "#### 3. Top Authours\n",
    "- Identify the top 5 authors by degree. What role do these node play in the network?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Identify the top 5 authors by degree\n",
    "top_authors = sorted(degrees.items(), key=lambda x: x[1], reverse=True)[:5]\n",
    "\n",
    "# Print the top authors and their degrees\n",
    "print(\"Top 5 Authors by Degree:\")\n",
    "for author, degree in top_authors:\n",
    "    print(f\"Author {author}: Degree {degree}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "- Research these authors online. What areas do they specialize in? Do you think that their work aligns with the themes of Computational Social Science? If not, what could be possible reasons? (answer in max 150 words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "CompSS",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
