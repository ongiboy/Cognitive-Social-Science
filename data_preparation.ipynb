{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "import tqdm\n",
    "import unicodedata\n",
    "import ast\n",
    "\n",
    "def normalize_name(name):\n",
    "    \"\"\"\n",
    "    returns more standard version of name\n",
    "        (removes accents, special characters, and makes lowercase)\n",
    "    \"\"\"\n",
    "\n",
    "    normalized_name = ''.join(c for c in unicodedata.normalize('NFD', name) if unicodedata.category(c) != 'Mn' and (c.isalnum() or c.isspace()))\n",
    "    return normalized_name.lower()\n",
    "\n",
    "\n",
    "def get_info_from_names(df_names):\n",
    "    \"\"\"\n",
    "    Retrieves author information from names (through a search)\n",
    "    \"\"\"\n",
    "\n",
    "    BASE_URL = 'https://api.openalex.org'\n",
    "    RESOURCE = '/authors'\n",
    "\n",
    "    data = []\n",
    "    for i in tqdm.tqdm(range(len(df_names))):\n",
    "        try:\n",
    "            response = requests.get(BASE_URL + RESOURCE, params={\"search\": df_names[\"name\"][i]})\n",
    "            all_results = response.json()[\"results\"]#[0]\n",
    "\n",
    "            names = []\n",
    "            normalized_df_name = normalize_name(df_names[\"name\"][i])\n",
    "\n",
    "            for r in range(len(all_results)):\n",
    "                name = all_results[r][\"display_name\"]\n",
    "                names.append(normalize_name(name))\n",
    "\n",
    "                if normalized_df_name in names:\n",
    "                    results = response.json()[\"results\"][r]\n",
    "                    break\n",
    "\n",
    "            if normalized_df_name not in names:\n",
    "                continue\n",
    "\n",
    "            data += [[results[\"id\"], results[\"display_name\"], results[\"works_api_url\"], results[\"summary_stats\"][\"h_index\"], results[\"works_count\"], results[\"last_known_institution\"][\"country_code\"]]]\n",
    "\n",
    "        except (IndexError, KeyError, ValueError, TypeError) as e:\n",
    "            print(f\"Skipping data point at iteration {i} due to error: {e}\")\n",
    "            continue  # Skip the rest of the loop and proceed to the next iteration\n",
    "        except requests.exceptions.RequestException as e:\n",
    "            print(f\"Request error at iteration {i}: {e}\")\n",
    "            continue\n",
    "\n",
    "    new_df = pd.DataFrame(data, columns=[\"id\",\"display_name\",\"works_api_url\",\"h_index\",\"works_count\", \"country_code\"])\n",
    "    return new_df\n",
    "\n",
    "def get_info_from_author_ids(author_ids):\n",
    "    \"\"\"\n",
    "    Gets info table from author ids.\n",
    "    \"\"\"\n",
    "\n",
    "    URL = 'https://api.openalex.org/authors'\n",
    "    author_ids = [id.split('/')[-1] for id in author_ids]\n",
    "\n",
    "    co_author_info = []\n",
    "    id_batches = [author_ids[i:i+25] for i in range(0, len(author_ids), 25)]\n",
    "\n",
    "    for i,ids_batch in tqdm.tqdm(enumerate(id_batches)):\n",
    "            \n",
    "        # search for result\n",
    "        params = {'filter': 'ids.openalex:'+'|'.join(ids_batch)}\n",
    "        result_batch = requests.get(URL, params=params).json()\n",
    "\n",
    "        # No results\n",
    "        if len(result_batch) == 0:\n",
    "            continue\n",
    "        \n",
    "        # Go through results (authors)\n",
    "        for result in result_batch['results']:\n",
    "\n",
    "            # if person doesnt have all info, skip person\n",
    "            try:\n",
    "                # extract desired information\n",
    "                person_info = [result['id'], result['display_name'], result['works_api_url'], result['summary_stats']['h_index'], result['works_count'], result['last_known_institution']['country_code']]\n",
    "            except:\n",
    "                continue\n",
    "\n",
    "            co_author_info.append(person_info)\n",
    "    \n",
    "    co_author_info_df = pd.DataFrame(co_author_info, columns=['id', 'display_name', 'works_api_url', 'h_index', 'works_count', 'country_code'])\n",
    "\n",
    "    return co_author_info_df\n",
    "\n",
    "def get_concept_ids(concept_requirements):\n",
    "    \"\"\"\n",
    "    Retrieves the OpenAlex concept_ids for a list of requirements.\n",
    "    \"\"\"\n",
    "\n",
    "    BASE_URL = 'https://api.openalex.org'\n",
    "    RESOURCE = '/concepts'\n",
    "\n",
    "    concept_ids = []\n",
    "    for concept in concept_requirements:\n",
    "        result = requests.get(BASE_URL + RESOURCE, params={'search': concept, 'filter': 'level:0'}).json()\n",
    "        concept_ids.append(result['results'][0]['id'])\n",
    "\n",
    "    concept_ids = [id.split(\"/\")[-1] for id in concept_ids]\n",
    "    return concept_ids\n",
    "\n",
    "\n",
    "def get_articles_from_authors(names, concept_ids_requirements_1, concept_ids_requirements_2, subset=False):\n",
    "    \"\"\"\n",
    "    Extracts articles from authors in the names table.\n",
    "        The articles are filtered by the criteria from the assignment description.\n",
    "    \"\"\"\n",
    "    \n",
    "    BASE_URL = 'https://api.openalex.org'\n",
    "    RESOURCE = '/works'\n",
    "\n",
    "    # Filter out authors not having 5-5000 works\n",
    "    names = names[(names['works_count']>=5) & (names['works_count']<=5000)]\n",
    "\n",
    "    table1 = []\n",
    "    table2 = []\n",
    "\n",
    "    # Search for articles in batches of 25 authors\n",
    "    name_batches = [list(names['id'][i:i+25]) for i in range(0, len(names), 25)]\n",
    "\n",
    "    for num_name_batch, name_batch in tqdm.tqdm(enumerate(name_batches)):\n",
    "\n",
    "        # short version for testing\n",
    "        if subset and num_name_batch>0:\n",
    "            break\n",
    "\n",
    "        # Scroll through the results\n",
    "        cursor = '*'\n",
    "        while True:\n",
    "            filters = ['cited_by_count:>10', \n",
    "                        'authors_count:<10',\n",
    "                        'authorships.author.id:'+'|'.join(name_batch),\n",
    "                        'concepts.id:'+'|'.join(concept_ids_requirements_1),\n",
    "                        'concepts.id:'+'|'.join(concept_ids_requirements_2)\n",
    "                        ]\n",
    "            parameters = {'per-page': 200,\n",
    "                            'filter': ','.join(filters),\n",
    "                            'cursor': cursor\n",
    "                            }\n",
    "            result = requests.get(BASE_URL + RESOURCE, params=parameters).json()\n",
    "\n",
    "            # If last page is reached (which is empty), break \n",
    "            cursor = result['meta']['next_cursor'] # next page for next search\n",
    "            if len(result['results'])==0 or cursor is None:\n",
    "                break\n",
    "\n",
    "            # Go through all articles and extract information\n",
    "            for n_article,article in enumerate(result['results']):\n",
    "                try:\n",
    "                    tab1 = [article['id'], article['publication_year'], article['cited_by_count'], [author['author']['id'] for author in article['authorships']]]\n",
    "                    tab2 = [article['id'], article['title'], article['abstract_inverted_index']]\n",
    "                    table1.append(tab1)\n",
    "                    table2.append(tab2)\n",
    "\n",
    "                except:\n",
    "                    print(\"skipped name batch:\", num_name_batch, \"article:\", n_article)\n",
    "                    continue\n",
    "    \n",
    "    table1 = pd.DataFrame(table1, columns=['id', 'publication_year', 'cited_by_count', 'authors'])\n",
    "    table2 = pd.DataFrame(table2, columns=['id', 'title', 'abstract_inverted_index'])\n",
    "\n",
    "    return table1, table2\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Turning names-list into the author dataset and saving as CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, turn names-list from Part 1 into a pandas dataframe\n",
    "names = pd.read_csv('data/names.csv')\n",
    "\n",
    "# Get info from names (table of authors + info)\n",
    "authors = get_info_from_names(names)\n",
    "\n",
    "# Drop duplicates\n",
    "authors = authors.drop_duplicates(subset='id')\n",
    "\n",
    "# Save authors to csv\n",
    "authors.to_csv(\"data/authors.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Expand the author dataset with co-authors and get the final author dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load authors and papers\n",
    "authors = pd.read_csv('data/authors.csv')\n",
    "papers = pd.read_csv('data/papers.csv')\n",
    "\n",
    "# Find the co-author IDs\n",
    "author_ids = authors[\"id\"]\n",
    "co_authors_ids = [x for x in papers[\"authors\"].copy().apply(ast.literal_eval).explode().dropna().unique() if x not in author_ids]\n",
    "\n",
    "# Get info from author ids\n",
    "co_author_info = get_info_from_author_ids(co_authors_ids)\n",
    "\n",
    "# Drop rows in co_author_info if \"nan\" in country_code\n",
    "co_author_info = co_author_info.dropna(subset=['country_code'])\n",
    "\n",
    "# Concatenate the original authors and the co-authors dataframes\n",
    "authors_final = pd.concat([authors, co_author_info]).drop_duplicates(subset=['id'])\n",
    "\n",
    "# Drop duplicates\n",
    "co_author_info = co_author_info.drop_duplicates(subset='id')\n",
    "\n",
    "# Save authors to csv\n",
    "co_author_info.to_csv(\"data/co_authors.csv\", index=False)\n",
    "authors_final.to_csv(\"data/authors_final.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Find only the co-authors works and combine them with the original authors' works"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define concept ids\n",
    "concepts_requirements_1 = ['Sociology', 'Psychology', 'Economics', 'Political Science']\n",
    "concepts_requirements_2 = ['Mathematics', 'Physics', 'Computer Science']\n",
    "concept_ids_1 = get_concept_ids(concepts_requirements_1)\n",
    "concept_ids_2 = get_concept_ids(concepts_requirements_2)\n",
    "\n",
    "# Get papers and abstracts from co-authors\n",
    "papers_from_co_authors, abstracts_from_co_authors = get_articles_from_authors(co_author_info, concept_ids_1, concept_ids_2, subset=False)\n",
    "\n",
    "# Concatenate the original papers/abstracts with co-authored papers/abstracts and drop duplicates\n",
    "papers = pd.read_csv('data/papers.csv')\n",
    "abstracts = pd.read_csv('data/abstracts.csv')\n",
    "papers_final = pd.concat([papers, papers_from_co_authors]).drop_duplicates(subset=['id'])\n",
    "abstracts_final = pd.concat([abstracts, abstracts_from_co_authors]).drop_duplicates(subset=['id'])\n",
    "\n",
    "# Save papers and abstracts to csv\n",
    "papers_final.to_csv(\"data/papers_final.csv\", index=False)\n",
    "# The final abstracts dataset is not saved in our Github, as it is very large.\n",
    "# abstracts_final.to_csv(\"data/abstracts_final.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remove authors who are not either IC2S2 authors or IC2S2 co-authors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "papers_final = pd.read_csv(\"data/papers_final.csv\")\n",
    "authors_final = pd.read_csv(\"data/authors_final.csv\")\n",
    "\n",
    "# Drop duplicates from papers\n",
    "papers_final = papers_final.drop_duplicates(subset=['id'])\n",
    "\n",
    "papers_final_copy = papers_final.copy()\n",
    "\n",
    "# Drop authors that are not in the authors_final dataframe\n",
    "for i, authors_list in tqdm.tqdm(enumerate(papers_final_copy[\"authors\"].apply(ast.literal_eval))):\n",
    "    new_list = []\n",
    "    for author in authors_list:\n",
    "        if author in authors_final[\"id\"].values:\n",
    "            new_list.append(author)\n",
    "            \n",
    "    papers_final_copy.at[i, \"authors\"] = new_list\n",
    "\n",
    "# Save papers to csv\n",
    "papers_final_copy.to_csv(\"data/papers_final.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "comsocsci2024",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
